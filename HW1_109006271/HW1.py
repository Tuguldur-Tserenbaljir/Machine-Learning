# -*- coding: utf-8 -*-
"""Machine_Learning_HW1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_nC5JI9PICBovsrhA7khNdRCxlNYrRef

Python Libraries
"""

import pandas as pd
import numpy as np
from scipy.stats import norm
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

"""Part 1. Splitting data into test and train.cvs"""

# Load the data
df = pd.read_csv('Wine.csv')

# Initialize empty lists to hold the training and testing data
train_data = []
test_data = []

# Iterate over each wine type 0,1,2
for wine_type in df['target'].unique():
    #Shuffle each wine catergory
    wine_type_data = df[df['target'] == wine_type].sample(frac=1, random_state=None)
    # Select the first 20 for testing and rest for trainning
    test_samples = wine_type_data.iloc[:20]
    train_samples = wine_type_data.iloc[20:]
    test_data.append(test_samples)
    train_data.append(train_samples)

# Concatenate the training and testing data
train_data = pd.concat(train_data).reset_index(drop=True)
test_data = pd.concat(test_data).reset_index(drop=True)

#Debugging
print("The length of the test data is: ")
print(len(test_data))

print("The length of the train data is: ")
print(len(train_data))

# Save the training and testing datasets with headers, only for the first time
train_data.to_csv('train.csv', index=False, header=True)
test_data.to_csv('test.csv', index=False, header=True)

"""Part 2. evaluate the posterior probabilities"""

# Calculate the Gaussian likelihood
def gaussian_likelihood(x, mean, var):
    return norm.pdf(x, mean, np.sqrt(var))

# Calculate the posterior probabilities
def calculate_posterior(x, priors):
    posteriors = []
    #print(f"Evaluating instance with features: {x.values}")
    for c in priors.index:
        prior = np.log(priors[c])
        likelihood = np.sum(np.log(gaussian_likelihood(x, means.loc[c], variances.loc[c])))
        posterior = prior + likelihood
        #print(f"Class {c}: Prior log probability = {prior:.4f}, Sum of log likelihood = {likelihood:.4f}, Posterior log probability = {posterior:.4f}")
        posteriors.append(posterior)
    return posteriors

# Load training and testing data
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

# Separate features and target
X_train = train_df.drop(columns=['target'])
y_train = train_df['target']
X_test = test_df.drop(columns=['target'])
y_test = test_df['target']

# Calculate the priors from the data
original_priors = y_train.value_counts(normalize=True).sort_index()
#print(original_priors)
# Testing for question 4
#modified_priors = pd.Series([1/3, 1/3, 1/3], index=original_priors.index)

# Calculate mean and variance for each feature per class
means = X_train.groupby(y_train).mean()
variances = X_train.groupby(y_train).var()

# Apply calculate_posterior with original and modified priors
predictions_original = X_test.apply(lambda x: calculate_posterior(x, original_priors), axis=1).apply(np.argmax)

# Testing for question 4
#predictions_modified = X_test.apply(lambda x: calculate_posterior(x, modified_priors), axis=1).apply(np.argmax)

# Calculate accuracy for original and modified priors
accuracy_original = (predictions_original == y_test).mean()

# Testing for question 4
#accuracy_modified = (predictions_modified == y_test).mean()

# Output the results
print(f'Original Accuracy: {accuracy_original:.2%}')
# Testing for question 4
#print(f'Modified Accuracy: {accuracy_modified:.2%}')

"""Part 3. PCA Visuals 2D"""

# Load the testing data
test_df = pd.read_csv('test.csv')

# Separate features and target
X_test = test_df.drop(columns=['target'])
y_test = test_df['target']

# Standardize the features
scaler = StandardScaler()
X_test_scaled = scaler.fit_transform(X_test)
#print(X_test_scaled)
#Set PCA to 2D
pca = PCA(n_components=2)
X_test_pca = pca.fit_transform(X_test_scaled)
#print(X_test_pca)
# Visualize PCA results
plt.figure(figsize=(10, 8))
targets = [0, 1, 2]
colors = ['r', 'g', 'b']
labels = ['Type 0', 'Type 1', 'Type 2']

#Plot data with respective data
for target, color, label in zip(targets, colors, labels):
    indicesToKeep = y_test == target
    plt.scatter(X_test_pca[indicesToKeep, 0], X_test_pca[indicesToKeep, 1], c=color, label=label, alpha=0.7)

#print(len(X_test_pca)) == 60
#print(X_test_pca), 60 pairs
#print(X_test_pca)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA Visualization of Testing Data')
plt.legend()
plt.grid(True)
plt.show()

"""Part 3. PCA Visuals 3D"""

#Set PCA to 3D
pca = PCA(n_components=3)
X_test_pca = pca.fit_transform(X_test_scaled)

# Visualize PCA results in 3D
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

targets = [0, 1, 2]
colors = ['r', 'g', 'b']
labels = ['Type 0', 'Type 1', 'Type 2']

#Plot data with respective data
for target, color, label in zip(targets, colors, labels):
    indicesToKeep = y_test == target
    ax.scatter(X_test_pca[indicesToKeep, 0], X_test_pca[indicesToKeep, 1], X_test_pca[indicesToKeep, 2],
               c=color, label=label, alpha=0.7)

ax.set_xlabel('Principal Component 1')
ax.set_ylabel('Principal Component 2')
ax.set_zlabel('Principal Component 3')
ax.set_title('PCA Visualization of Testing Data in 3D')
ax.legend()
plt.show()

"""Part 5. Confusion Matrix"""

cm = confusion_matrix(y_test, predictions_original)

# Plot the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()